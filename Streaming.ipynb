{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODK6IqnAuTJYTD9bOQSNdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar045/Assignment-For-Filed/blob/main/Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CajOH4lqDfR9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import asyncio\n",
        "from typing import Any\n",
        "\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Body\n",
        "from fastapi.responses import StreamingResponse\n",
        "from queue import Queue\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
        "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
        "from langchain.schema import LLMResult\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# initialize the agent (we need to do this for the callbacks)\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    temperature=0.0,\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    streaming=True,  # ! important\n",
        "    callbacks=[]  # ! important (but we will add them later)\n",
        ")\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    k=5,\n",
        "    return_messages=True,\n",
        "    output_key=\"output\"\n",
        ")\n",
        "agent = initialize_agent(\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    tools=[],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    return_intermediate_steps=False\n",
        ")\n",
        "\n",
        "class AsyncCallbackHandler(AsyncIteratorCallbackHandler):\n",
        "    content: str = \"\"\n",
        "    final_answer: bool = False\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
        "        self.content += token\n",
        "        # if we passed the final answer, we put tokens in queue\n",
        "        if self.final_answer:\n",
        "            if '\"action_input\": \"' in self.content:\n",
        "                if token not in ['\"', \"}\"]:\n",
        "                    self.queue.put_nowait(token)\n",
        "        elif \"Final Answer\" in self.content:\n",
        "            self.final_answer = True\n",
        "            self.content = \"\"\n",
        "\n",
        "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        if self.final_answer:\n",
        "            self.content = \"\"\n",
        "            self.final_answer = False\n",
        "            self.done.set()\n",
        "        else:\n",
        "            self.content = \"\"\n",
        "\n",
        "async def run_call(query: str, stream_it: AsyncCallbackHandler):\n",
        "    # assign callback handler\n",
        "    agent.agent.llm_chain.llm.callbacks = [stream_it]\n",
        "    # now query\n",
        "    await agent.acall(inputs={\"input\": query})\n",
        "\n",
        "# request input format\n",
        "class Query(BaseModel):\n",
        "    text: str\n",
        "\n",
        "async def create_gen(query: str, stream_it: AsyncCallbackHandler):\n",
        "    task = asyncio.create_task(run_call(query, stream_it))\n",
        "    async for token in stream_it.aiter():\n",
        "        yield token\n",
        "    await task\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(\n",
        "    query: Query = Body(...),\n",
        "):\n",
        "    stream_it = AsyncCallbackHandler()\n",
        "    gen = create_gen(query.text, stream_it)\n",
        "    return StreamingResponse(gen, media_type=\"text/event-stream\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    \"\"\"Check the api is running\"\"\"\n",
        "    return {\"status\": \"ðŸ¤™\"}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\n",
        "        \"app:app\",\n",
        "        host=\"localhost\",\n",
        "        port=8000,\n",
        "        reload=True\n",
        "    )"
      ]
    }
  ]
}