{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHoFkUJCsXzeBjrTClrczd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar045/Assignment-For-Filed/blob/main/Elastic_Search_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JdkzH4TAQ_q",
        "outputId": "03f52dd5-5eb4-43d3-b164-8b9507a04c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document indexed with ID: B056422fd in index: business\n",
            "Elasticsearch response: {'_index': 'business', '_id': 'B056422fd', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import operator\n",
        "from elasticsearch import Elasticsearch\n",
        "from datetime import datetime\n",
        "from PyPDF2 import PdfReader\n",
        "import hashlib\n",
        "\n",
        "# Step 0: Extract Text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    pdf_reader = PdfReader(open(pdf_path, \"rb\"))\n",
        "    text = \"\"\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Step 1: Zero-Shot Classification to Determine Topic\n",
        "def classify_document(document_text):\n",
        "    classifier = pipeline(\"zero-shot-classification\")\n",
        "    candidate_labels = [\"education\", \"politics\", \"business\", \"crime\"]\n",
        "    results = classifier(document_text, candidate_labels)\n",
        "    max_index, _ = max(enumerate(results['scores']), key=operator.itemgetter(1))\n",
        "    top_label = results['labels'][max_index]\n",
        "    return top_label\n",
        "\n",
        "# Step 2 and 3: Generate Document ID and Index Name\n",
        "def generate_doc_id_and_index(top_label, document_text, file_name, timestamp):\n",
        "    unique_string = f\"{document_text}{file_name}{timestamp}\"\n",
        "    sha256_hash = hashlib.sha256(unique_string.encode()).hexdigest()\n",
        "    doc_id = f\"{top_label[0].upper()}{sha256_hash[:8]}\"  # Using first 8 characters of hash for brevity\n",
        "    index_name = f\"{top_label}\"\n",
        "    return doc_id, index_name\n",
        "\n",
        "# Step 4: Index Document in Elasticsearch\n",
        "def index_document_in_elasticsearch(doc_id, index_name, document_text, file_name, timestamp):\n",
        "\n",
        "    document = {\n",
        "        \"text\": document_text,\n",
        "        \"file_name\": file_name,\n",
        "        \"timestamp\": timestamp\n",
        "    }\n",
        "    response = client.index(\n",
        "        index=index_name,\n",
        "        id=doc_id,\n",
        "        document=document\n",
        "    )\n",
        "    return response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to your PDF file\n",
        "    pdf_path = \"/content/DWSample2-PDF.pdf\"\n",
        "\n",
        "    # Extract text from PDF\n",
        "    document_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Classify the document\n",
        "    top_label = classify_document(document_text)\n",
        "\n",
        "    # Additional metadata\n",
        "    file_name = pdf_path.split(\"/\")[-1]  # Extracting file name from path\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "    # Generate a unique document ID and index name\n",
        "    doc_id, index_name = generate_doc_id_and_index(top_label, document_text, file_name, timestamp)\n",
        "\n",
        "    # Index the document in Elasticsearch\n",
        "    response = index_document_in_elasticsearch(doc_id, index_name, document_text, file_name, timestamp)\n",
        "\n",
        "    print(f\"Document indexed with ID: {doc_id} in index: {index_name}\")\n",
        "    print(f\"Elasticsearch response: {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "# Step 1: Search Elasticsearch to get document ID and index\n",
        "def search_document(query, index_name):\n",
        "    response = client.search(index=index_name, body={\n",
        "        \"query\": {\n",
        "            \"match\": query\n",
        "        }\n",
        "    })\n",
        "    if response['hits']['total']['value'] > 0:\n",
        "        return response['hits']['hits'][0]['_id'], response['hits']['hits'][0]['_index']\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Step 2: Retrieve the document using the document ID and index name\n",
        "def retrieve_document(doc_id, index_name):\n",
        "    response = client.get(index=index_name, id=doc_id)\n",
        "    return response['_source']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your query and index\n",
        "    query = {\"text\": \"Featured on the Food Network\"}\n",
        "    index_name = \"business\"\n",
        "\n",
        "    # Search for the document\n",
        "    doc_id, found_index_name = search_document(query, index_name)\n",
        "\n",
        "    if doc_id and found_index_name:\n",
        "        # Retrieve the document\n",
        "        document = retrieve_document(doc_id, found_index_name)\n",
        "\n",
        "        # Extract file name\n",
        "        file_name = document.get(\"file_name\", \"\")\n",
        "\n",
        "        if file_name:\n",
        "            print(f\"The text belongs to the document named {file_name}.\")\n",
        "        else:\n",
        "            print(\"File name not found in the document.\")\n",
        "    else:\n",
        "        print(\"No document found based on the search criteria.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPvyd0_3AVNG",
        "outputId": "21e497c1-7bd8-4143-dfa4-d36c138a21c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text belongs to the document named DWSample2-PDF.pdf.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-ec7eb38a61f6>:5: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
            "  response = client.search(index=index_name, body={\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import operator\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "# Function to classify text and determine index name\n",
        "def classify_text(text):\n",
        "    classifier = pipeline(\"zero-shot-classification\")\n",
        "    candidate_labels = [\"education\", \"politics\", \"business\", \"crime\"]\n",
        "    results = classifier(text, candidate_labels)\n",
        "    max_index, _ = max(enumerate(results['scores']), key=operator.itemgetter(1))\n",
        "    top_label = results['labels'][max_index]\n",
        "    return top_label\n",
        "\n",
        "# Search Elasticsearch to get document ID and index\n",
        "def search_document(query, index_name):\n",
        "    response = client.search(index=index_name, body={\n",
        "        \"query\": {\n",
        "            \"match\": query\n",
        "        }\n",
        "    })\n",
        "    if response['hits']['total']['value'] > 0:\n",
        "        return response['hits']['hits'][0]['_id'], response['hits']['hits'][0]['_index']\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Retrieve the document using the document ID and index name\n",
        "def retrieve_document(doc_id, index_name):\n",
        "    response = client.get(index=index_name, id=doc_id)\n",
        "    return response['_source']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your query text\n",
        "    query_text = \"Featured on the Food Network\"\n",
        "\n",
        "    # Classify the query text to determine the index name\n",
        "    top_label = classify_text(query_text)\n",
        "    index_name = f\"{top_label}\"\n",
        "\n",
        "    # Search for the document\n",
        "    doc_id, found_index_name = search_document({\"text\": query_text}, index_name)\n",
        "\n",
        "    if doc_id and found_index_name:\n",
        "        # Retrieve the document\n",
        "        document = retrieve_document(doc_id, found_index_name)\n",
        "\n",
        "        # Extract file name\n",
        "        file_name = document.get(\"file_name\", \"\")\n",
        "\n",
        "        if file_name:\n",
        "            print(f\"The text belongs to the document named {file_name}.\")\n",
        "        else:\n",
        "            print(\"File name not found in the document.\")\n",
        "    else:\n",
        "        print(\"No document found based on the search criteria.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCQ5HlgICDPX",
        "outputId": "051ca81f-2f87-4bf9-a105-9e5af33de569"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text belongs to the document named DWSample2-PDF.pdf.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-d482e668cab0>:16: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
            "  response = client.search(index=index_name, body={\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify text and determine index name\n",
        "def classify_text(text):\n",
        "    classifier = pipeline(\"zero-shot-classification\")\n",
        "    candidate_labels = [\"education\", \"politics\", \"business\", \"crime\"]\n",
        "    results = classifier(text, candidate_labels)\n",
        "    max_index, _ = max(enumerate(results['scores']), key=operator.itemgetter(1))\n",
        "    top_label = results['labels'][max_index]\n",
        "    return top_label\n",
        "\n",
        "# Search Elasticsearch to find documents related to query text\n",
        "def search_related_documents(query_text):\n",
        "    # Classify the query text to determine the index name\n",
        "    top_label = classify_text(query_text)\n",
        "    index_name = f\"{top_label}\"\n",
        "\n",
        "    response = client.search(index=index_name, body={\n",
        "        \"query\": {\n",
        "            \"match\": {\n",
        "                \"text\": query_text\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # If there are matching documents, return their IDs and file names\n",
        "    if response['hits']['total']['value'] > 0:\n",
        "        related_documents = []\n",
        "        for hit in response['hits']['hits']:\n",
        "            doc_id = hit['_id']\n",
        "            file_name = hit['_source'].get(\"file_name\", \"\")\n",
        "            related_documents.append((doc_id, file_name))\n",
        "        return related_documents\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your query text\n",
        "    query_text = \"Featured on the Food Network\"\n",
        "\n",
        "    # Search for related documents\n",
        "    related_documents = search_related_documents(query_text)\n",
        "\n",
        "    if related_documents:\n",
        "        print(f\"Found {len(related_documents)} documents related to the query:\")\n",
        "        for doc_id, file_name in related_documents:\n",
        "            print(f\"Document ID: {doc_id}, File Name: {file_name}\")\n",
        "    else:\n",
        "        print(\"No documents found related to the query.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7IfXh21EtYp",
        "outputId": "cb67faea-9e8d-4ed0-ac2a-f2b4baf70e4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 documents related to the query:\n",
            "Document ID: B056422fd, File Name: DWSample2-PDF.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-db035adfb7c8>:16: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
            "  response = client.search(index=index_name, body={\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import operator\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "results = classifier(\n",
        "  '''In her complaint, the woman alleged that the IAS officer raped her on the pretext of marriage in March, 2020. Back then, he was not an IAS officer, but promised to marry her once he is selected for the post, the complaint says.\n",
        "\n",
        "\"I bore the expenses for Yuvraj's IAS preparation and took care of him when he went into depression on getting selected as an IAS officer. He said his family would never disapprove of our marriage if he cleared the Union Public Service Commission (UPSC) exam,\" the woman said in her complaint.''',\n",
        "  candidate_labels=[\"education\", \"politics\", \"business\",\"crime\"]\n",
        ")\n",
        "\n",
        "print(results)\n",
        "# Get the index of the max score\n",
        "max_index, max_value = max(enumerate(results['scores']), key=operator.itemgetter(1))\n",
        "\n",
        "# Extract the label using the index\n",
        "top_label = results['labels'][max_index]\n",
        "\n",
        "print(top_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8S4PminDAV5",
        "outputId": "a0ec70b1-c91e-4e06-c095-3a7ccca1a0af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': 'In her complaint, the woman alleged that the IAS officer raped her on the pretext of marriage in March, 2020. Back then, he was not an IAS officer, but promised to marry her once he is selected for the post, the complaint says.\\n\\n\"I bore the expenses for Yuvraj\\'s IAS preparation and took care of him when he went into depression on getting selected as an IAS officer. He said his family would never disapprove of our marriage if he cleared the Union Public Service Commission (UPSC) exam,\" the woman said in her complaint.', 'labels': ['crime', 'business', 'politics', 'education'], 'scores': [0.7683209180831909, 0.13174334168434143, 0.06714367121458054, 0.03279207646846771]}\n",
            "crime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Elasticsearch Python client\n",
        "!pip install elasticsearch PyPDF2\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3JAWVsUFslY",
        "outputId": "b6c05298-ab0d-42d8-c232-16d5fe4abe4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.9.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.5/395.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elastic-transport<9,>=8 (from elasticsearch)\n",
            "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2,>=1.26.2 (from elastic-transport<9,>=8->elasticsearch)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2023.7.22)\n",
            "Installing collected packages: urllib3, PyPDF2, elastic-transport, elasticsearch\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "Successfully installed PyPDF2-3.0.1 elastic-transport-8.4.0 elasticsearch-8.9.0 urllib3-1.26.16\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece])\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 evaluate-0.4.0 huggingface-hub-0.17.1 multiprocess-0.70.15 responses-0.18.0 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.33.1 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "# Password for the 'elastic' user generated by Elasticsearch\n",
        "ELASTIC_PASSWORD = \"jHkppEE4Go9QnA2piAyjC2fZ\"\n",
        "\n",
        "# Found in the 'Manage Deployment' page\n",
        "CLOUD_ID = \"7a974c9b987b419c8d019d2f0185e544:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRjYWJhMmQwNDZhMDM0OTBlYTIyMjE0ODBmZjIxYjgxNCRiODlhZGEwNTM2YzU0NjgyODc0ZWFkZTVjNzhiNjIzYw==\"\n",
        "\n",
        "# Create the client instance\n",
        "client = Elasticsearch(\n",
        "    cloud_id=CLOUD_ID,\n",
        "    basic_auth=(\"elastic\", ELASTIC_PASSWORD)\n",
        ")\n",
        "\n",
        "# Successful response!\n",
        "client.info()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRnmI_OnF-IK",
        "outputId": "a6f1ec9c-12cb-4448-9897-05b0154fc269"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'name': 'instance-0000000001', 'cluster_name': 'caba2d046a03490ea2221480ff21b814', 'cluster_uuid': 'biL8xjXxSfS64h8IQaOLag', 'version': {'number': '8.9.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'e8179018838f55b8820685f92e245abef3bddc0f', 'build_date': '2023-08-31T02:43:14.210479707Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.index(\n",
        "    index=\"my_index\",\n",
        "    id=\"my_document_id\",\n",
        "    document={\n",
        "        \"foo\": \"foo\",\n",
        "        \"bar\": \"bar\",\n",
        "    }\n",
        ")\n",
        "\n",
        "client.get(index=\"my_index\", id=\"my_document_id\")\n",
        "\n",
        "\n",
        "client.search(index=\"my_index\", query={\n",
        "    \"match\": {\n",
        "        \"foo\": \"foo\"\n",
        "    }\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hq3CLl7HGt1",
        "outputId": "92a86217-af76-4b29-feba-94d966280e34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'took': 0, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}