{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar045/Assignment-For-Filed/blob/main/ludwig_llama2_7b_finetuning_4bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama2-7b Fine-Tuning with 4bit Quantization\n",
        "\n",
        "We recommend using a GPU runtime for this example. In the Colab menu bar, choose Runtime > Change Runtime Type and choose GPU under Hardware Accelerator."
      ],
      "metadata": {
        "id": "GhfRuYMVyWh_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnBtciuBP9hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Ludwig\n",
        "\n",
        "We'll use the latest version of Ludwig which includes support for quantized fine-tuning."
      ],
      "metadata": {
        "id": "5tbiuqdEyhj0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYazZYnssfXQ"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow --quiet\n",
        "!pip install \"git+https://github.com/ludwig-ai/ludwig.git#egg=ludwig[llm]\" --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set your HuggingFace API Token\n",
        "\n",
        "Obtain a [HuggingFace API Token](https://huggingface.co/docs/hub/security-tokens) and request access to [Llama2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) before proceeding."
      ],
      "metadata": {
        "id": "D-rbreKEyoeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"<api_token>\""
      ],
      "metadata": {
        "id": "Y3gUrr7XvHSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Model Training\n",
        "\n",
        "The Ludwig [configuration](https://ludwig.ai/latest/configuration/) specifies the components of the training job including:\n",
        "\n",
        "- Model type (LLM) and base pretrained model name from HuggingFace\n",
        "- Input and output features from the training dataset\n",
        "- Quantization (4bit) and parameter-efficient fine-tuning (LoRA)\n",
        "- Training hyperparameters (learning rate, batch size, etc.)\n",
        "- Preprocessing (e.g., sampling to speed up training)\n",
        "- Backend for execution (local, but could also be Ray)"
      ],
      "metadata": {
        "id": "P7q67YNKy7g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "config_str = \"\"\"\n",
        "model_type: llm\n",
        "base_model: meta-llama/Llama-2-7b-hf\n",
        "\n",
        "input_features:\n",
        "  - name: instruction\n",
        "    type: text\n",
        "\n",
        "output_features:\n",
        "  - name: output\n",
        "    type: text\n",
        "\n",
        "quantization:\n",
        "  bits: 4\n",
        "\n",
        "adapter:\n",
        "  type: lora\n",
        "\n",
        "trainer:\n",
        "  type: finetune\n",
        "  learning_rate: 0.0003\n",
        "  batch_size: 1\n",
        "  gradient_accumulation_steps: 8\n",
        "  epochs: 3\n",
        "  learning_rate_scheduler:\n",
        "    warmup_fraction: 0.01\n",
        "\n",
        "preprocessing:\n",
        "  sample_ratio: 0.01\n",
        "\n",
        "backend:\n",
        "  type: local\n",
        "\"\"\"\n",
        "\n",
        "config = yaml.safe_load(config_str)"
      ],
      "metadata": {
        "id": "GKie6b70tTkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train!\n",
        "\n",
        "Start training on your local GPU and monitor progress (including metrics) inline.\n",
        "\n",
        "In this example, we'll be training on the [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) dataset to turn Llama2-7b into a rudimentary chatbot. But you can use any dataset to fine-tune for other tasks."
      ],
      "metadata": {
        "id": "IgkHvOz7zfIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from ludwig.api import LudwigModel\n",
        "\n",
        "\n",
        "model = LudwigModel(config=config, logging_level=logging.INFO)\n",
        "results = model.train(dataset=\"ludwig://alpaca\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "JfZq1-qbulcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's next?\n",
        "\n",
        "Now you can save / export your model to HuggingFace or explore [Predibase](https://predibase.com/) for serving and managed infrastructure for training larger LLMs and other model types."
      ],
      "metadata": {
        "id": "nlEPUBFaztU0"
      }
    }
  ]
}