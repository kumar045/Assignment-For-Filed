{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjm/bpoG3703U+aArVPBEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar045/Assignment-For-Filed/blob/main/Unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sX2VlisQUIu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from typing import List, Dict\n",
        "import tiktoken\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "\n",
        "# Dataset generation functions\n",
        "\n",
        "def generate_dataset(task_description: str, task_examples: List[Dict[str, str]], num_examples: int) -> List[Dict[str, str]]:\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(num_examples):\n",
        "        example = random.choice(task_examples)\n",
        "        instruction = example.get('instruction', task_description)\n",
        "        input_text = example.get('input', '')\n",
        "        output = example.get('output', '')\n",
        "\n",
        "        dataset_item = {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": input_text,\n",
        "            \"output\": output\n",
        "        }\n",
        "        dataset.append(dataset_item)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def save_dataset(dataset: List[Dict[str, str]], filename: str):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_dataset(filename: str) -> List[Dict[str, str]]:\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def validate_dataset(dataset: List[Dict[str, str]], max_tokens: int = 2048) -> List[Dict[str, str]]:\n",
        "    valid_dataset = []\n",
        "    for item in dataset:\n",
        "        total_tokens = sum(count_tokens(item[key]) for key in [\"instruction\", \"input\", \"output\"])\n",
        "        if total_tokens <= max_tokens:\n",
        "            valid_dataset.append(item)\n",
        "    return valid_dataset\n",
        "\n",
        "# Training functions\n",
        "\n",
        "def prepare_dataset_for_training(dataset: List[Dict[str, str]]) -> Dataset:\n",
        "    return Dataset.from_list(dataset)\n",
        "\n",
        "def setup_model_and_tokenizer(model_name: str, max_seq_length: int):\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "        use_rslora=False,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "def train_model(model, tokenizer, dataset, output_dir, num_train_epochs, per_device_train_batch_size):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not FastLanguageModel.is_bfloat16_supported(),\n",
        "        bf16=FastLanguageModel.is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=tokenizer.model_max_length,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return trainer\n",
        "\n",
        "def save_model(trainer, output_dir):\n",
        "    trainer.save_model(output_dir)\n",
        "\n",
        "def push_to_hub(model, tokenizer, repo_name, hf_token=None):\n",
        "    if hf_token:\n",
        "        HfFolder.save_token(hf_token)\n",
        "\n",
        "    model.push_to_hub(repo_name, use_auth_token=True)\n",
        "    tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
        "\n",
        "    return f\"Model and tokenizer pushed to Hugging Face Hub: {repo_name}\"\n",
        "\n",
        "def quantize_model(model, tokenizer, output_dir, quantization_methods):\n",
        "    gguf_paths = []\n",
        "    for method in quantization_methods:\n",
        "        print(f\"Quantizing model to {method}...\")\n",
        "        gguf_path = model.save_pretrained_gguf(output_dir, tokenizer, quantization_method=method)\n",
        "        gguf_paths.append(gguf_path)\n",
        "        print(f\"Quantized model ({method}) saved to {gguf_path}\")\n",
        "    return gguf_paths\n",
        "\n",
        "def create_ollama_modelfile(gguf_path, model_name):\n",
        "    modelfile_content = f\"\"\"\n",
        "FROM {gguf_path}\n",
        "TEMPLATE \"\"\"Below are some instructions that describe a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{{{{ .Prompt }}}}\n",
        "{{{{ if .Input }}}}\n",
        "### Input:\n",
        "{{{{ .Input }}}}\n",
        "{{{{ end }}}}\n",
        "### Response:\n",
        "{{{{ .Response }}}}\"\"\"\n",
        "PARAMETER stop \"<|start_header_id|>\"\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "PARAMETER stop \"<|end_header_id|>\"\n",
        "PARAMETER stop \"<|end_of_text|>\"\n",
        "PARAMETER stop \"<|reserved_special_token_\"\n",
        "\"\"\"\n",
        "    modelfile_path = os.path.join(os.path.dirname(gguf_path), f\"Modelfile_{os.path.basename(gguf_path)}\")\n",
        "    with open(modelfile_path, 'w') as f:\n",
        "        f.write(modelfile_content)\n",
        "    print(f\"Ollama Modelfile created at {modelfile_path}\")\n",
        "    return modelfile_path\n",
        "\n",
        "def convert_to_ollama(gguf_paths, model_name):\n",
        "    ollama_model_names = []\n",
        "    for gguf_path in gguf_paths:\n",
        "        modelfile_path = create_ollama_modelfile(gguf_path, model_name)\n",
        "        quant_method = os.path.basename(gguf_path).split('.')[1].lower()\n",
        "        ollama_model_name = f\"unsloth_{model_name.replace('-', '_').lower()}_{quant_method}\"\n",
        "\n",
        "        print(f\"Converting model to Ollama format with name: {ollama_model_name}\")\n",
        "        subprocess.run([\"ollama\", \"create\", ollama_model_name, \"-f\", modelfile_path], check=True)\n",
        "        print(f\"Model converted and added to Ollama with name: {ollama_model_name}\")\n",
        "        ollama_model_names.append(ollama_model_name)\n",
        "    return ollama_model_names\n",
        "\n",
        "# Gradio UI functions\n",
        "\n",
        "def generate_response(instruction, input_text, ollama_model_name):\n",
        "    command = [\n",
        "        \"ollama\", \"run\", ollama_model_name,\n",
        "        f\"Below are some instructions that describe a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\"\n",
        "    ]\n",
        "    if input_text:\n",
        "        command[-1] += f\"### Input:\\n{input_text}\\n\"\n",
        "    command[-1] += \"### Response:\"\n",
        "\n",
        "    result = subprocess.run(command, capture_output=True, text=True)\n",
        "    return result.stdout.strip()\n",
        "\n",
        "def process_pipeline(task_description, example1_input, example1_output, example2_input, example2_output,\n",
        "                     task_examples, num_examples, max_tokens, model_name, num_train_epochs,\n",
        "                     per_device_train_batch_size, output_dir, push_to_hub, hub_model_id, hub_token, quant_versions):\n",
        "    # Prepare examples\n",
        "    examples = [\n",
        "        {\"instruction\": task_description, \"input\": example1_input, \"output\": example1_output},\n",
        "        {\"instruction\": task_description, \"input\": example2_input, \"output\": example2_output}\n",
        "    ]\n",
        "\n",
        "    if task_examples:\n",
        "        examples.extend(json.loads(task_examples))\n",
        "\n",
        "    # Generate dataset\n",
        "    dataset = generate_dataset(task_description, examples, num_examples)\n",
        "    valid_dataset = validate_dataset(dataset, max_tokens)\n",
        "\n",
        "    output_file = f\"{task_description.replace(' ', '_')[:30]}_dataset.json\"\n",
        "    save_dataset(valid_dataset, output_file)\n",
        "\n",
        "    status = f\"Dataset with {len(valid_dataset)} valid examples (out of {len(dataset)} generated) has been saved to {output_file}\\n\"\n",
        "    status += f\"Examples removed due to token limit: {len(dataset) - len(valid_dataset)}\\n\"\n",
        "\n",
        "    # Prepare dataset for training\n",
        "    train_dataset = prepare_dataset_for_training(valid_dataset)\n",
        "\n",
        "    # Setup model and tokenizer\n",
        "    model, tokenizer = setup_model_and_tokenizer(model_name, max_tokens)\n",
        "\n",
        "    # Train model\n",
        "    trainer = train_model(model, tokenizer, train_dataset, output_dir, num_train_epochs, per_device_train_batch_size)\n",
        "\n",
        "    # Save model locally\n",
        "    save_model(trainer, output_dir)\n",
        "    status += f\"Fine-tuned model has been saved to {output_dir}\\n\"\n",
        "\n",
        "    # Push to Hugging Face Hub if requested\n",
        "    if push_to_hub:\n",
        "        if not hub_model_id:\n",
        "            status += \"Error: Please provide a Hub Model ID to push to Hugging Face Hub.\\n\"\n",
        "        else:\n",
        "            status += push_to_hub(model, tokenizer, hub_model_id, hub_token) + \"\\n\"\n",
        "\n",
        "    # Quantize model\n",
        "    quant_output_dir = os.path.join(output_dir, \"quantized\")\n",
        "    os.makedirs(quant_output_dir, exist_ok=True)\n",
        "\n",
        "    quantization_methods = []\n",
        "    if \"Q4\" in quant_versions:\n",
        "        quantization_methods.append(\"q4_k_m\")\n",
        "    if \"Q8\" in quant_versions:\n",
        "        quantization_methods.append(\"q8_0\")\n",
        "\n",
        "    gguf_paths = quantize_model(model, tokenizer, quant_output_dir, quantization_methods)\n",
        "\n",
        "    # Convert to Ollama\n",
        "    ollama_model_names = convert_to_ollama(gguf_paths, model_name.split('/')[-1])\n",
        "\n",
        "    status += \"Quantization and conversion completed.\\n\"\n",
        "    for name in ollama_model_names:\n",
        "        status += f\"Model converted and added to Ollama with name: {name}\\n\"\n",
        "\n",
        "    return status, ollama_model_names\n",
        "\n",
        "def main_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# Flexible LLM Fine-tuning Pipeline with Multiple Quantization Options\")\n",
        "\n",
        "        with gr.Tab(\"Setup and Training\"):\n",
        "            task_description = gr.Textbox(label=\"Task Description\", placeholder=\"e.g., Simplify the given text, or Translate the text to French\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    example1_input = gr.Textbox(label=\"Example 1 Input\", placeholder=\"Enter the input for the first example\")\n",
        "                    example1_output = gr.Textbox(label=\"Example 1 Output\", placeholder=\"Enter the expected output for the first example\")\n",
        "                with gr.Column():\n",
        "                    example2_input = gr.Textbox(label=\"Example 2 Input\", placeholder=\"Enter the input for the second example\")\n",
        "                    example2_output = gr.Textbox(label=\"Example 2 Output\", placeholder=\"Enter the expected output for the second example\")\n",
        "\n",
        "            task_examples = gr.Textbox(label=\"Additional Task Examples (JSON format)\", placeholder='''[\n",
        "    {\"instruction\": \"Simplify this text:\", \"input\": \"The quantum mechanical model is based on mathematics of waves and introduces the concept of the electron cloud.\", \"output\": \"The quantum model of atoms uses math about waves. It says electrons move around the atom like a cloud.\"},\n",
        "    {\"instruction\": \"Translate to French:\", \"input\": \"Hello, how are you?\", \"output\": \"Bonjour, comment allez-vous ?\"}\n",
        "]''')\n",
        "            num_examples = gr.Slider(minimum=10, maximum=1000, step=10, label=\"Number of examples to generate\", value=100)\n",
        "            max_tokens = gr.Slider(minimum=128, maximum=4096, step=128, label=\"Maximum tokens per example\", value=2048)\n",
        "            model_name = gr.Dropdown([\"unsloth/llama-3-8b-bnb-4bit\", \"unsloth/mistral-7b-bnb-4bit\"], label=\"Pretrained model name\")\n",
        "            num_train_epochs = gr.Slider(minimum=1, maximum=10, step=1, label=\"Number of training epochs\", value=3)\n",
        "            per_device_train_batch_size = gr.Slider(minimum=1, maximum=32, step=1, label=\"Batch size per device\", value=2)\n",
        "            output_dir = gr.Textbox(label=\"Output directory for fine-tuned model\", value=\"./fine_tuned_model\")\n",
        "            push_to_hub = gr.Checkbox(label=\"Push to Hugging Face Hub\")\n",
        "            hub_model_id = gr.Textbox(label=\"Hugging Face Hub Model ID (e.g., 'username/model-name')\")\n",
        "            hub_token = gr.Textbox(label=\"Hugging Face Hub API Token\", type=\"password\")\n",
        "            quant_versions = gr.CheckboxGroup([\"Q4\", \"Q8\"], label=\"Quantization Versions\", value=[\"Q4\"])\n",
        "\n",
        "            start_button = gr.Button(\"Start Pipeline\")\n",
        "            output = gr.Textbox(label=\"Pipeline Status\")\n",
        "            chat_button = gr.Button(\"Open Chat Interface\")\n",
        "\n",
        "        with gr.Tab(\"Chat\"):\n",
        "            model_select = gr.Dropdown(label=\"Select Ollama Model\")\n",
        "            with gr.Row():\n",
        "                instruction = gr.Textbox(label=\"Instruction\", placeholder=\"Enter your instruction here...\")\n",
        "                input_text = gr.Textbox(label=\"Input (optional)\", placeholder=\"Enter input text if needed...\")\n",
        "            chat_output = gr.Textbox(label=\"Model Output\")\n",
        "            chat_button = gr.Button(\"Generate Response\")\n",
        "\n",
        "        def update_model_list():\n",
        "            result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
        "            models = [line.split()[0] for line in result.stdout.strip().split('\\n')[1:]]  # Skip header\n",
        "            return gr.Dropdown.update(choices=models)\n",
        "\n",
        "        def start_pipeline(*args):\n",
        "            status, ollama_model_names = process_pipeline(*args)\n",
        "            return status, gr.Button.update(interactive=True), update_model_list()\n",
        "\n",
        "        start_button.click(start_pipeline,\n",
        "                           inputs=[task_description, example1_input, example1_output, example2_input, example2_output,\n",
        "                                   task_examples, num_examples, max_tokens, model_name, num_train_epochs,\n",
        "                                   per_device_train_batch_size, output_dir, push_to_hub, hub_model_id, hub_token, quant_versions],\n",
        "                           outputs=[output, chat_button, model_select])\n",
        "\n",
        "        def chat(instruction, input_text, model_name):\n",
        "            if not model_name:\n",
        "                return \"Please select a model first.\"\n",
        "            return generate_response(instruction, input_text, model_name)\n",
        "\n",
        "        chat_button.click(chat, inputs=[instruction, input_text, model_select], outputs=[chat_output])\n",
        "\n",
        "    demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_interface()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from typing import List, Dict\n",
        "import tiktoken\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel\n",
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import anthropic\n",
        "import google.generativeai as genai\n",
        "import openai\n",
        "\n",
        "# Configuration for AI models\n",
        "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize AI models\n",
        "claude = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Unsloth model setup\n",
        "unsloth_models = {\n",
        "    \"llama3.1\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"mistral\": \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "}\n",
        "\n",
        "# Dataset generation functions\n",
        "\n",
        "def generate_dataset_with_ai(task_description: str, examples: List[Dict[str, str]], num_examples: int, ai_model: str) -> List[Dict[str, str]]:\n",
        "    dataset = []\n",
        "\n",
        "    for _ in range(num_examples):\n",
        "        if ai_model == \"claude\":\n",
        "            new_example = generate_with_claude(task_description, examples)\n",
        "        elif ai_model == \"gemini\":\n",
        "            new_example = generate_with_gemini(task_description, examples)\n",
        "        elif ai_model == \"openai\":\n",
        "            new_example = generate_with_openai(task_description, examples)\n",
        "        elif ai_model in [\"llama3.1\", \"mistral\"]:\n",
        "            new_example = generate_with_unsloth(task_description, examples, ai_model)\n",
        "        else:\n",
        "            new_example = random.choice(examples)  # Fallback to random selection\n",
        "\n",
        "        dataset.append(new_example)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def generate_with_claude(task_description: str, examples: List[Dict[str, str]]) -> Dict[str, str]:\n",
        "    prompt = f\"Task: {task_description}\\n\\nExamples:\\n\"\n",
        "    for ex in examples[:2]:  # Use first two examples\n",
        "        prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "    prompt += \"Generate a new, unique example following the same pattern:\"\n",
        "\n",
        "    response = claude.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        prompt=prompt,\n",
        "        max_tokens_to_sample=300\n",
        "    )\n",
        "\n",
        "    # Parse Claude's response\n",
        "    lines = response.completion.strip().split('\\n')\n",
        "    new_input = lines[0].replace(\"Input: \", \"\")\n",
        "    new_output = lines[1].replace(\"Output: \", \"\")\n",
        "\n",
        "    return {\"instruction\": task_description, \"input\": new_input, \"output\": new_output}\n",
        "\n",
        "def generate_with_gemini(task_description: str, examples: List[Dict[str, str]]) -> Dict[str, str]:\n",
        "    prompt = f\"Task: {task_description}\\n\\nExamples:\\n\"\n",
        "    for ex in examples[:2]:  # Use first two examples\n",
        "        prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "    prompt += \"Generate a new, unique example following the same pattern. Respond with just the Input and Output, no additional text.\"\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Parse Gemini's response\n",
        "    lines = response.text.strip().split('\\n')\n",
        "    new_input = lines[0].replace(\"Input: \", \"\")\n",
        "    new_output = lines[1].replace(\"Output: \", \"\")\n",
        "\n",
        "    return {\"instruction\": task_description, \"input\": new_input, \"output\": new_output}\n",
        "\n",
        "def generate_with_openai(task_description: str, examples: List[Dict[str, str]]) -> Dict[str, str]:\n",
        "    prompt = f\"Task: {task_description}\\n\\nExamples:\\n\"\n",
        "    for ex in examples[:2]:  # Use first two examples\n",
        "        prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "    prompt += \"Generate a new, unique example following the same pattern. Respond with just the Input and Output, no additional text.\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    # Parse OpenAI's response\n",
        "    lines = response.choices[0].message.content.strip().split('\\n')\n",
        "    new_input = lines[0].replace(\"Input: \", \"\")\n",
        "    new_output = lines[1].replace(\"Output: \", \"\")\n",
        "\n",
        "    return {\"instruction\": task_description, \"input\": new_input, \"output\": new_output}\n",
        "\n",
        "def generate_with_unsloth(task_description: str, examples: List[Dict[str, str]], model_type: str) -> Dict[str, str]:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=unsloth_models[model_type],\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    prompt = f\"Task: {task_description}\\n\\nExamples:\\n\"\n",
        "    for ex in examples[:2]:  # Use first two examples\n",
        "        prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "    prompt += \"Generate a new, unique example following the same pattern. Respond with just the Input and Output, no additional text.\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
        "    output = model.generate(input_ids, max_new_tokens=100)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Parse Unsloth model's response\n",
        "    lines = response.strip().split('\\n')\n",
        "    new_input = lines[-2].replace(\"Input: \", \"\")\n",
        "    new_output = lines[-1].replace(\"Output: \", \"\")\n",
        "\n",
        "    return {\"instruction\": task_description, \"input\": new_input, \"output\": new_output}\n",
        "\n",
        "def save_dataset(dataset: List[Dict[str, str]], filename: str):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_dataset(filename: str) -> List[Dict[str, str]]:\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def validate_dataset(dataset: List[Dict[str, str]], max_tokens: int = 2048) -> List[Dict[str, str]]:\n",
        "    valid_dataset = []\n",
        "    for item in dataset:\n",
        "        total_tokens = sum(count_tokens(item[key]) for key in [\"instruction\", \"input\", \"output\"])\n",
        "        if total_tokens <= max_tokens:\n",
        "            valid_dataset.append(item)\n",
        "    return valid_dataset\n",
        "\n",
        "# Training functions\n",
        "\n",
        "def prepare_dataset_for_training(dataset: List[Dict[str, str]]) -> Dataset:\n",
        "    return Dataset.from_list(dataset)\n",
        "\n",
        "def setup_model_and_tokenizer(model_name: str, max_seq_length: int):\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "        use_rslora=False,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "def train_model(model, tokenizer, dataset, output_dir, num_train_epochs, per_device_train_batch_size):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not FastLanguageModel.is_bfloat16_supported(),\n",
        "        bf16=FastLanguageModel.is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=tokenizer.model_max_length,\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return trainer\n",
        "\n",
        "def save_model(trainer, output_dir):\n",
        "    trainer.save_model(output_dir)\n",
        "\n",
        "def push_to_hub(model, tokenizer, repo_name, hf_token=None):\n",
        "    if hf_token:\n",
        "        HfFolder.save_token(hf_token)\n",
        "\n",
        "    model.push_to_hub(repo_name, use_auth_token=True)\n",
        "    tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
        "\n",
        "    return f\"Model and tokenizer pushed to Hugging Face Hub: {repo_name}\"\n",
        "\n",
        "def quantize_model(model, tokenizer, output_dir, quantization_methods):\n",
        "    gguf_paths = []\n",
        "    for method in quantization_methods:\n",
        "        print(f\"Quantizing model to {method}...\")\n",
        "        gguf_path = model.save_pretrained_gguf(output_dir, tokenizer, quantization_method=method)\n",
        "        gguf_paths.append(gguf_path)\n",
        "        print(f\"Quantized model ({method}) saved to {gguf_path}\")\n",
        "    return gguf_paths\n",
        "\n",
        "def create_ollama_modelfile(gguf_path, model_name):\n",
        "    modelfile_content = f\"\"\"\n",
        "FROM {gguf_path}\n",
        "TEMPLATE \"\"\"Below are some instructions that describe a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{{{{ .Prompt }}}}\n",
        "{{{{ if .Input }}}}\n",
        "### Input:\n",
        "{{{{ .Input }}}}\n",
        "{{{{ end }}}}\n",
        "### Response:\n",
        "{{{{ .Response }}}}\"\"\"\n",
        "PARAMETER stop \"<|start_header_id|>\"\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "PARAMETER stop \"<|end_header_id|>\"\n",
        "PARAMETER stop \"<|end_of_text|>\"\n",
        "PARAMETER stop \"<|reserved_special_token_\"\n",
        "\"\"\"\n",
        "    modelfile_path = os.path.join(os.path.dirname(gguf_path), f\"Modelfile_{os.path.basename(gguf_path)}\")\n",
        "    with open(modelfile_path, 'w') as f:\n",
        "        f.write(modelfile_content)\n",
        "    print(f\"Ollama Modelfile created at {modelfile_path}\")\n",
        "    return modelfile_path\n",
        "\n",
        "def convert_to_ollama(gguf_paths, model_name):\n",
        "    ollama_model_names = []\n",
        "    for gguf_path in gguf_paths:\n",
        "        modelfile_path = create_ollama_modelfile(gguf_path, model_name)\n",
        "        quant_method = os.path.basename(gguf_path).split('.')[1].lower()\n",
        "        ollama_model_name = f\"unsloth_{model_name.replace('-', '_').lower()}_{quant_method}\"\n",
        "\n",
        "        print(f\"Converting model to Ollama format with name: {ollama_model_name}\")\n",
        "        subprocess.run([\"ollama\", \"create\", ollama_model_name, \"-f\", modelfile_path], check=True)\n",
        "        print(f\"Model converted and added to Ollama with name: {ollama_model_name}\")\n",
        "        ollama_model_names.append(ollama_model_name)\n",
        "    return ollama_model_names\n",
        "\n",
        "# Gradio UI functions\n",
        "\n",
        "def generate_response(instruction, input_text, ollama_model_name):\n",
        "    command = [\n",
        "        \"ollama\", \"run\", ollama_model_name,\n",
        "        f\"Below are some instructions that describe a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\"\n",
        "    ]\n",
        "    if input_text:\n",
        "        command[-1] += f\"### Input:\\n{input_text}\\n\"\n",
        "    command[-1] += \"### Response:\"\n",
        "\n",
        "    result = subprocess.run(command, capture_output=True, text=True)\n",
        "    return result.stdout.strip()\n",
        "\n",
        "def process_pipeline(task_description, example1_input, example1_output, example2_input, example2_output,\n",
        "                     task_examples, num_examples, max_tokens, model_name, num_train_epochs,\n",
        "                     per_device_train_batch_size, output_dir, push_to_hub, hub_model_id, hub_token,\n",
        "                     quant_versions, dataset_generation_model):\n",
        "    # Prepare examples\n",
        "    examples = [\n",
        "        {\"instruction\": task_description, \"input\": example1_input, \"output\": example1_output},\n",
        "        {\"instruction\": task_description, \"input\": example2_input, \"output\": example2_output}\n",
        "    ]\n",
        "\n",
        "    if task_examples:\n",
        "        examples.extend(json.loads(task_examples))\n",
        "\n",
        "    # Generate dataset\n",
        "    dataset = generate_dataset_with_ai(task_description, examples, num_examples, dataset_generation_model)\n",
        "\n",
        "    valid_dataset = validate_dataset(dataset, max_tokens)\n",
        "\n",
        "    output_file = f\"{task_description.replace(' ', '_')[:30]}_dataset.json\"\n",
        "    save_dataset(valid_dataset, output_file)\n",
        "\n",
        "    status = f\"Dataset with {len(valid_dataset)} valid examples (out of {len(dataset)} generated) has been saved to {output_file}\\n\"\n",
        "    status += f\"Examples removed due to token limit: {len(dataset) - len(valid_dataset)}\\n\"\n",
        "\n",
        "    # Prepare dataset for training\n",
        "    train_dataset = prepare_dataset_for_training(valid_dataset)\n",
        "\n",
        "    # Setup model and tokenizer\n",
        "    model, tokenizer = setup_model_and_tokenizer(model_name, max_tokens)\n",
        "\n",
        "    # Train model\n",
        "    trainer = train_model(model, tokenizer, train_dataset, output_dir, num_train_epochs, per_device_train_batch_size)\n",
        "\n",
        "    # Save model locally\n",
        "    save_model(trainer, output_dir)\n",
        "    status += f\"Fine-tuned model has been saved to {output_dir}\\n\"\n",
        "\n",
        "    # Push to Hugging Face Hub if requested\n",
        "    # Push to Hugging Face Hub if requested\n",
        "    if push_to_hub:\n",
        "        if not hub_model_id:\n",
        "            status += \"Error: Please provide a Hub Model ID to push to Hugging Face Hub.\\n\"\n",
        "        else:\n",
        "            status += push_to_hub(model, tokenizer, hub_model_id, hub_token) + \"\\n\"\n",
        "\n",
        "    # Quantize model\n",
        "    quant_output_dir = os.path.join(output_dir, \"quantized\")\n",
        "    os.makedirs(quant_output_dir, exist_ok=True)\n",
        "\n",
        "    quantization_methods = []\n",
        "    if \"Q4\" in quant_versions:\n",
        "        quantization_methods.append(\"q4_k_m\")\n",
        "    if \"Q8\" in quant_versions:\n",
        "        quantization_methods.append(\"q8_0\")\n",
        "\n",
        "    gguf_paths = quantize_model(model, tokenizer, quant_output_dir, quantization_methods)\n",
        "\n",
        "    # Convert to Ollama\n",
        "    ollama_model_names = convert_to_ollama(gguf_paths, model_name.split('/')[-1])\n",
        "\n",
        "    status += \"Quantization and conversion completed.\\n\"\n",
        "    for name in ollama_model_names:\n",
        "        status += f\"Model converted and added to Ollama with name: {name}\\n\"\n",
        "\n",
        "    return status, ollama_model_names\n",
        "\n",
        "def main_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# Flexible LLM Fine-tuning Pipeline with AI-Assisted Dataset Generation\")\n",
        "\n",
        "        with gr.Tab(\"Setup and Training\"):\n",
        "            task_description = gr.Textbox(label=\"Task Description\", placeholder=\"e.g., Simplify the given text, or Translate the text to French\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    example1_input = gr.Textbox(label=\"Example 1 Input\", placeholder=\"Enter the input for the first example\")\n",
        "                    example1_output = gr.Textbox(label=\"Example 1 Output\", placeholder=\"Enter the expected output for the first example\")\n",
        "                with gr.Column():\n",
        "                    example2_input = gr.Textbox(label=\"Example 2 Input\", placeholder=\"Enter the input for the second example\")\n",
        "                    example2_output = gr.Textbox(label=\"Example 2 Output\", placeholder=\"Enter the expected output for the second example\")\n",
        "\n",
        "            task_examples = gr.Textbox(label=\"Additional Task Examples (JSON format)\", placeholder='''[\n",
        "    {\"instruction\": \"Simplify this text:\", \"input\": \"The quantum mechanical model is based on mathematics of waves and introduces the concept of the electron cloud.\", \"output\": \"The quantum model of atoms uses math about waves. It says electrons move around the atom like a cloud.\"},\n",
        "    {\"instruction\": \"Translate to French:\", \"input\": \"Hello, how are you?\", \"output\": \"Bonjour, comment allez-vous ?\"}\n",
        "]''')\n",
        "            num_examples = gr.Slider(minimum=10, maximum=1000, step=10, label=\"Number of examples to generate\", value=100)\n",
        "            dataset_generation_model = gr.Radio([\"self\", \"claude\", \"gemini\", \"openai\", \"llama3.1\", \"mistral\"], label=\"Dataset Generation Model\", value=\"self\")\n",
        "            max_tokens = gr.Slider(minimum=128, maximum=4096, step=128, label=\"Maximum tokens per example\", value=2048)\n",
        "            model_name = gr.Dropdown(list(unsloth_models.values()), label=\"Pretrained model name\")\n",
        "            num_train_epochs = gr.Slider(minimum=1, maximum=10, step=1, label=\"Number of training epochs\", value=3)\n",
        "            per_device_train_batch_size = gr.Slider(minimum=1, maximum=32, step=1, label=\"Batch size per device\", value=2)\n",
        "            output_dir = gr.Textbox(label=\"Output directory for fine-tuned model\", value=\"./fine_tuned_model\")\n",
        "            push_to_hub = gr.Checkbox(label=\"Push to Hugging Face Hub\")\n",
        "            hub_model_id = gr.Textbox(label=\"Hugging Face Hub Model ID (e.g., 'username/model-name')\")\n",
        "            hub_token = gr.Textbox(label=\"Hugging Face Hub API Token\", type=\"password\")\n",
        "            quant_versions = gr.CheckboxGroup([\"Q4\", \"Q8\"], label=\"Quantization Versions\", value=[\"Q4\"])\n",
        "\n",
        "            start_button = gr.Button(\"Start Pipeline\")\n",
        "            output = gr.Textbox(label=\"Pipeline Status\")\n",
        "            chat_button = gr.Button(\"Open Chat Interface\")\n",
        "\n",
        "        with gr.Tab(\"Chat\"):\n",
        "            model_select = gr.Dropdown(label=\"Select Ollama Model\")\n",
        "            with gr.Row():\n",
        "                instruction = gr.Textbox(label=\"Instruction\", placeholder=\"Enter your instruction here...\")\n",
        "                input_text = gr.Textbox(label=\"Input (optional)\", placeholder=\"Enter input text if needed...\")\n",
        "            chat_output = gr.Textbox(label=\"Model Output\")\n",
        "            chat_button = gr.Button(\"Generate Response\")\n",
        "\n",
        "        def update_model_list():\n",
        "            result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
        "            models = [line.split()[0] for line in result.stdout.strip().split('\\n')[1:]]  # Skip header\n",
        "            return gr.Dropdown.update(choices=models)\n",
        "\n",
        "        def start_pipeline(*args):\n",
        "            status, ollama_model_names = process_pipeline(*args)\n",
        "            return status, gr.Button.update(interactive=True), update_model_list()\n",
        "\n",
        "        start_button.click(start_pipeline,\n",
        "                           inputs=[task_description, example1_input, example1_output, example2_input, example2_output,\n",
        "                                   task_examples, num_examples, max_tokens, model_name, num_train_epochs,\n",
        "                                   per_device_train_batch_size, output_dir, push_to_hub, hub_model_id, hub_token,\n",
        "                                   quant_versions, dataset_generation_model],\n",
        "                           outputs=[output, chat_button, model_select])\n",
        "\n",
        "        def chat(instruction, input_text, model_name):\n",
        "            if not model_name:\n",
        "                return \"Please select a model first.\"\n",
        "            return generate_response(instruction, input_text, model_name)\n",
        "\n",
        "        chat_button.click(chat, inputs=[instruction, input_text, model_select], outputs=[chat_output])\n",
        "\n",
        "    demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_interface()"
      ],
      "metadata": {
        "id": "3z5h-hndTKDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}